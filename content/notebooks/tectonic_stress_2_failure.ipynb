{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inversion for tectonic stresses\n",
    "## Step 2: Analysis of friction and fluid pressure at failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import halfspace.projections as hsp\n",
    "import time\n",
    "\n",
    "sys.path.append('../aux_scripts/')\n",
    "import stress_comps_vectorized as scv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_poster_file = '../results/baloch_rake_posteriors.csv'\n",
    "out_name = '../results/baloch_fail_posteriors.csv'\n",
    "fault_file = '../test_data/baloch_fault_model.csv'\n",
    "\n",
    "fault_df = pd.read_csv(fault_file, index_col=0)\n",
    "r_priors_df = pd.read_csv(rake_poster_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some initial constants\n",
    "n_trials = r_priors_df.shape[0]\n",
    "n_points = len(fault_df.index)\n",
    "rho = 2700\n",
    "g = 9.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(70)\n",
    "\n",
    "r_priors = r_priors_df[['txx', 'tyy', 'txy']].values # make numpy array of priors\n",
    "phi_priors = np.random.random(1e4)\n",
    "phi_priors = phi_priors[r_priors_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe\n",
    "search_df_cols = ['iter', 'txx', 'tyy', 'txy', 'mu', 'phi', 'pt_index', \n",
    "                  'depth', 'strike', 'dip', 'slip_m']\n",
    "\n",
    "iter_range = np.float_(r_priors_df.index.values) \n",
    "pt_range = np.arange(n_points, dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make list of priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_range = np.float_(r_priors_df.index.values) \n",
    "pt_range = np.arange(n_points, dtype='float')\n",
    "\n",
    "index_list = [[iter_ind, phi_priors[i], r_priors[i,0], \n",
    "               r_priors[i,1], r_priors[i,2], pi]\n",
    "              for i, iter_ind in enumerate(iter_range) for pi in pt_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_array = np.array(index_list)\n",
    "del index_list\n",
    "\n",
    "iter_index = np.int_(index_array[:,0].copy() )\n",
    "pt_index = np.int_(index_array[:,5].copy() )\n",
    "phi_prior_array = index_array[:,1]\n",
    "t_prior_array = index_array[:,2:5]\n",
    "del index_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df=pd.DataFrame(index=np.arange(len(iter_index)), \n",
    "                       columns=search_df_cols, dtype=float)\n",
    "\n",
    "search_df['iter'] = iter_index\n",
    "search_df['phi'] = phi_prior_array\n",
    "search_df['pt_index'] = pt_index\n",
    "search_df[['txx', 'tyy', 'txy']] = t_prior_array\n",
    "\n",
    "search_df['mxx'] = 0.\n",
    "search_df['mxy'] = 0.\n",
    "search_df['mxz'] = 0.\n",
    "search_df['myy'] = 0.\n",
    "search_df['myz'] = 0.\n",
    "search_df['mzz'] = 0.\n",
    "\n",
    "# This is a list of the columns in the search_df that we are going to\n",
    "# fill with values from the fault slip dataframe.\n",
    "df_fill_cols = ['depth', 'strike', 'dip', 'slip_m',\n",
    "                'mxx', 'myy', 'mxy', 'mzz', 'mxz', 'myz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This (`df_copy_cols`) might require a little configuration if the names in the fault dataframe are different than those in this example.  But there needs to be a 1:1 correspondance between the column name and order in `df_fill_cols` and `df_copy_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_cols = ['z', 'strike','dip','slip_m',\n",
    "                'xx_stress', 'yy_stress', 'xy_stress', 'zz_stress',\n",
    "                'xz_stress', 'yz_stress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_array = fault_df[df_copy_cols].values    # copy columns\n",
    "df_reps = np.tile(df_col_array, [n_trials, 1])  # make sequential copies\n",
    "search_df[df_fill_cols] = df_reps               # fill df\n",
    "search_df.depth *= 1000                         # make depth into m (orig km)\n",
    "\n",
    "del df_reps # save some RAM\n",
    "\n",
    "# convert from MPa to Pa\n",
    "search_df[['mxx', 'myy', 'mxy', 'mzz', 'mxz', 'myz']] *= 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc fault stresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df['tau_s'] = scv.strike_shear( strike=search_df.strike,\n",
    "                                      dip=search_df.dip, rho=rho, g=g,\n",
    "                                      mxx=search_df.mxx, myy=search_df.myy,\n",
    "                                      mxy=search_df.mxy, mzz=search_df.mzz,\n",
    "                                      mxz=search_df.mxz, myz=search_df.myz,\n",
    "                                      txx=search_df.txx, tyy=search_df.tyy,\n",
    "                                      txy=search_df.txy, depth=search_df.depth)\n",
    "\n",
    "search_df['tau_d'] = scv.dip_shear( strike=search_df.strike,\n",
    "                                      dip=search_df.dip, rho=rho, g=g,\n",
    "                                      mxx=search_df.mxx, myy=search_df.myy,\n",
    "                                      mxy=search_df.mxy, mzz=search_df.mzz,\n",
    "                                      mxz=search_df.mxz, myz=search_df.myz,\n",
    "                                      txx=search_df.txx, tyy=search_df.tyy,\n",
    "                                      txy=search_df.txy, depth=search_df.depth)\n",
    "\n",
    "search_df['sig_n_eff'] = scv.eff_normal_stress( strike=search_df.strike,\n",
    "                                      dip=search_df.dip, rho=rho, g=g,\n",
    "                                      mxx=search_df.mxx, myy=search_df.myy,\n",
    "                                      mxy=search_df.mxy, mzz=search_df.mzz,\n",
    "                                      mxz=search_df.mxz, myz=search_df.myz,\n",
    "                                      txx=search_df.txx, tyy=search_df.tyy,\n",
    "                                      txy=search_df.txy, depth=search_df.depth,\n",
    "                                      phi=search_df.phi)\n",
    "\n",
    "search_df['tau_mag'] = np.sqrt(search_df.tau_s**2 + search_df.tau_d**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate likelihoods\n",
    "\n",
    "### Calculate misfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weighted misfits, start filtering\n",
    "mean_slip = fault_df.slip_m.mean()\n",
    "\n",
    "search_df['weighted_tau_misfit']=search_df.tau_mag *search_df.slip_m /mean_slip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = search_df.groupby('iter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $\\mu$ at failure for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_iter = iters.weighted_tau_misfit.mean() / iters.sig_n_eff.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter samples that have $0 \\leq \\mu \\leq 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_real = mu_iter[(0 <= mu_iter) & (mu_iter <= 1)] # make filter\n",
    "\n",
    "phi_iters = iters.phi.mean()               # get phi for each iteration\n",
    "\n",
    "phi_keep = phi_iters[mu_real.index]        # filter phi samples\n",
    "txx_keep = iters.txx.mean()[mu_real.index] # filter stresses and likelihood\n",
    "tyy_keep = iters.tyy.mean()[mu_real.index]\n",
    "txy_keep = iters.txy.mean()[mu_real.index]\n",
    "likelihood_keep = r_priors_df.loc[mu_real.index, 'likelihood']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now save filtered posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_posteriors = pd.concat([txx_keep, tyy_keep, txy_keep, mu_real, phi_keep,\n",
    "                             likelihood_keep], axis=1)\n",
    "\n",
    "fail_posteriors.columns = ['txx','tyy','txy','mu','phi', 'likelihood']\n",
    "\n",
    "fail_posteriors.to_csv(out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
